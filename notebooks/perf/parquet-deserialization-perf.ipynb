{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet Deserialization Benchmarks\n",
    "\n",
    "To determine if it is likely that spark is introducing undue latency in simple counting/aggregation operations, this notebook will use [parquet4s](https://github.com/mjakubowski84/parquet4s) to read and time parquet dumps of plink data directly.\n",
    "\n",
    "**Conclusion**: Spark takes upwards of 10 seconds to traverse one 100k record partition of a vcf-esque dataset.  Other operations that do a little more than simply traverse a file, like those in the Glow GWAS tutorial for counting unique variant/sample ids, take somewhere between 10 and 20 seconds so it appears that simply deserializing the information is taking a large chunk of the time (50+%).  Using a direct parquet reader was slower than Spark, so that foray adds little information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T20:17:29.661012Z",
     "start_time": "2020-01-13T20:17:24.494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/01/13 23:07:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$           , paths._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$               , benchmark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrame\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.github.mjakubowski84.parquet4s.ParquetReader\n",
       "\u001b[39m\n",
       "\u001b[36mdata_dir\u001b[39m: \u001b[32mbetter\u001b[39m.\u001b[32mfiles\u001b[39m.\u001b[32mFile\u001b[39m = /home/eczech/data/gwas/tutorial/1_QC_GWAS\n",
       "\u001b[36mpath\u001b[39m: \u001b[32mbetter\u001b[39m.\u001b[32mfiles\u001b[39m.\u001b[32mFile\u001b[39m = /home/eczech/data/gwas/tutorial/1_QC_GWAS/HapMap_3_r3_1.parquet\n",
       "\u001b[36mss\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@5ed9e86a\n",
       "\u001b[32mimport \u001b[39m\u001b[36mss.implicits._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.github.mjakubowski84::parquet4s-core:1.0.0`\n",
    "import $ivy.`sh.almond::almond-spark:0.6.0`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.4.4`\n",
    "import $file.^.init.paths, paths._\n",
    "import $file.^.init.benchmark, benchmark._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import com.github.mjakubowski84.parquet4s.ParquetReader\n",
    "Logger.getLogger(\"org\").setLevel(Level.WARN)\n",
    "\n",
    "val data_dir = GWAS_TUTORIAL_DATA_DIR / \"1_QC_GWAS\"\n",
    "val path = data_dir / \"HapMap_3_r3_1.parquet\"\n",
    "\n",
    "val ss = {\n",
    "  NotebookSparkSession\n",
    "    .builder()\n",
    "    .progress(enable=false, keep=false)\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "    .config(\"spark.ui.enabled\", \"false\")\n",
    "    .config(\"spark.driver.host\", \"localhost\")\n",
    "    .master(\"local[1]\") // Use single threaded reads for comparisons\n",
    "    .getOrCreate()\n",
    "}\n",
    "import ss.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T19:23:50.925535Z",
     "start_time": "2020-01-13T19:23:42.816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contigName: string (nullable = true)\n",
      " |-- names: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- position: double (nullable = true)\n",
      " |-- start: long (nullable = true)\n",
      " |-- end: long (nullable = true)\n",
      " |-- referenceAllele: string (nullable = true)\n",
      " |-- alternateAlleles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- genotypes: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- sampleId: string (nullable = true)\n",
      " |    |    |-- calls: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.read.parquet(path.toString).printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfile\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/eczech/data/gwas/tutorial/1_QC_GWAS/HapMap_3_r3_1.parquet/part-00005-72073dbc-7b2b-49c4-91c5-a14cdb8b553d-c000.snappy.parquet\"\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select a single file to compare file deserialization times on\n",
    "val file = path.glob(\"*.parquet\").toList(0).toString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres3\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m99865L\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.read.parquet(file.toString).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 15.5 seconds\n",
      "Elapsed time: 15.4 seconds\n",
      "Elapsed time: 15.7 seconds\n",
      "Elapsed time: 15.5 seconds\n",
      "Elapsed time: 15.4 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mRecord\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Test reads on a projection with two scalar (i.e. small) fields\n",
    "case class Record (\n",
    "    contigName: Option[String],\n",
    "    position: Option[Double]\n",
    ")\n",
    "\n",
    "(1 to 5).foreach(_ => time {\n",
    "    val records = ParquetReader.read[Record](file.toString)\n",
    "    records.size // traverse once\n",
    "    records.close()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T20:17:37.195251Z",
     "start_time": "2020-01-13T20:17:35.217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 29.7 seconds\n",
      "Elapsed time: 29.8 seconds\n",
      "Elapsed time: 29.7 seconds\n",
      "Elapsed time: 29.8 seconds\n",
      "Elapsed time: 30.0 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mGenotype\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mRecord\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Test traversal over full projection\n",
    "case class Genotype (\n",
    "    sampleId: Option[String],\n",
    "    calls: Array[Int]\n",
    ")\n",
    "case class Record (\n",
    "    contigName: Option[String],\n",
    "    names: Option[Array[String]],\n",
    "    position: Option[Double],\n",
    "    start: Option[Long],\n",
    "    end: Option[Long],\n",
    "    referenceAllele: Option[String],\n",
    "    alternateAlleles: Option[Array[String]],\n",
    "    genotypes: Option[Array[Genotype]]\n",
    ")\n",
    "\n",
    "(1 to 5).foreach(_ => time {\n",
    "    val records = ParquetReader.read[Record](file.toString)\n",
    "    records.size // traverse once\n",
    "    records.close()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T20:21:34.294501Z",
     "start_time": "2020-01-13T20:17:40.212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 10.2 seconds\n",
      "Elapsed time: 9.2 seconds\n",
      "Elapsed time: 9.2 seconds\n",
      "Elapsed time: 9.1 seconds\n",
      "Elapsed time: 9.2 seconds\n"
     ]
    }
   ],
   "source": [
    "// Compare to spark times by forcing spark to compute some stupid\n",
    "// aggregation that involves many of the fields, especially the genotypes\n",
    "// field since it is by far the largest\n",
    "(1 to 5).foreach(_ => time {\n",
    "    ss.read.parquet(file.toString)\n",
    "    .agg(\n",
    "        max(length($\"contigName\")) + \n",
    "        sum($\"position\") + \n",
    "        sum(size($\"genotypes\")) + \n",
    "        sum(size($\"names\")) + \n",
    "        sum(size($\"alternateAlleles\"))\n",
    "    )\n",
    "    .collect()\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
