{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/eczech/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/eczech/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/01/17 22:00:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$           , spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$           , paths._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$          , glow._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mio.projectglow.Glow\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[36mss\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@1f020a3d\n",
       "\u001b[32mimport \u001b[39m\u001b[36mss.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mdata_dir\u001b[39m: \u001b[32mbetter\u001b[39m.\u001b[32mfiles\u001b[39m.\u001b[32mFile\u001b[39m = /home/eczech/data/gwas/tutorial/2_PS_GWAS"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.^.init.spark, spark._\n",
    "import $file.^.init.paths, paths._\n",
    "import $file.^.init.glow, glow._\n",
    "import io.projectglow.Glow\n",
    "import org.apache.spark.sql.functions._\n",
    "val ss = getLocalSparkSession()\n",
    "import ss.implicits._\n",
    "Glow.register(ss)\n",
    "val data_dir = GWAS_TUTORIAL_DATA_DIR / \"2_PS_GWAS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.read.format(\"vcf\").load(data_dir / \"ALL.2of4intersection.20100804.genotypes.vcf.gz\" toString)\n",
    "    .write.parquet(data_dir / \"ALL.2of4intersection.20100804.genotypes.parquet\" toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/01/16 14:55:40 INFO PlinkFileFormat$: hlsUsage:[plinkRead,{\"includeSampleIds\":true,\"mergeFidIid\":true}]\n",
      "20/01/16 14:55:42 INFO PlinkFileFormat: Reading variants [1094036, 1458715]\n",
      "20/01/16 14:55:43 INFO PlinkFileFormat: Reading variants [364679, 729358]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [0, 364679]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [729358, 1094036]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [1823393, 2188072]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [1458715, 1823393]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [3646786, 4011464]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [3282107, 3646786]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [2188072, 2552750]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [2917429, 3282107]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [4011464, 4376143]\n",
      "20/01/16 14:55:45 INFO PlinkFileFormat: Reading variants [5470178, 5808310]\n",
      "20/01/16 14:55:45 INFO PlinkFileFormat: Reading variants [2552750, 2917429]\n",
      "20/01/16 14:55:45 INFO PlinkFileFormat: Reading variants [4740821, 5105500]\n",
      "20/01/16 14:55:45 INFO PlinkFileFormat: Reading variants [4376143, 4740821]\n",
      "20/01/16 14:55:45 INFO PlinkFileFormat: Reading variants [5105500, 5470178]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres12\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m5808310L\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.read.format(\"plink\").load(data_dir / \"1kG_MDS5.bed\" toString).drop(\"genotypes\").count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/01/16 13:24:03 INFO PlinkFileFormat$: hlsUsage:[plinkRead,{\"includeSampleIds\":true,\"mergeFidIid\":true}]\n",
      "20/01/16 13:24:06 INFO PlinkFileFormat: Reading variants [0, 364679]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [364679, 729358]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [729358, 1094036]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [1458715, 1823393]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [1094036, 1458715]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [1823393, 2188072]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [2552750, 2917429]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [2188072, 2552750]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [2917429, 3282107]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [4011464, 4376143]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [3646786, 4011464]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [3282107, 3646786]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [4376143, 4740821]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [5105500, 5470178]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [4740821, 5105500]\n",
      "20/01/16 13:24:08 INFO PlinkFileFormat: Reading variants [5470178, 5808310]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres2\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m5808310L\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.read.format(\"plink\").load(data_dir / \"1kG_MDS5.bed\" toString).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/01/16 13:17:05 INFO PlinkFileFormat$: hlsUsage:[plinkRead,{\"includeSampleIds\":true,\"mergeFidIid\":true}]\n",
      "20/01/16 13:17:12 INFO PlinkFileFormat: Reading variants [0, 849480]\n",
      "20/01/16 13:17:12 INFO PlinkFileFormat: Reading variants [849480, 1698959]\n",
      "20/01/16 13:17:13 INFO PlinkFileFormat: Reading variants [1698959, 2548438]\n",
      "20/01/16 13:17:17 INFO PlinkFileFormat: Reading variants [2548438, 3397918]\n",
      "20/01/16 13:17:17 INFO PlinkFileFormat: Reading variants [4247397, 5096876]\n",
      "20/01/16 13:17:17 INFO PlinkFileFormat: Reading variants [3397918, 4247397]\n",
      "20/01/16 13:17:17 INFO PlinkFileFormat: Reading variants [5096876, 5946356]\n",
      "20/01/16 13:17:18 INFO PlinkFileFormat: Reading variants [5946356, 6795835]\n",
      "20/01/16 13:17:18 INFO PlinkFileFormat: Reading variants [7645314, 8494793]\n",
      "20/01/16 13:17:18 INFO PlinkFileFormat: Reading variants [6795835, 7645314]\n",
      "20/01/16 13:17:18 INFO PlinkFileFormat: Reading variants [8494793, 9344273]\n",
      "20/01/16 13:17:18 INFO PlinkFileFormat: Reading variants [9344273, 10193752]\n",
      "20/01/16 13:17:19 INFO PlinkFileFormat: Reading variants [10193752, 11043231]\n",
      "20/01/16 13:17:19 INFO PlinkFileFormat: Reading variants [11892711, 12742190]\n",
      "20/01/16 13:17:19 INFO PlinkFileFormat: Reading variants [11043231, 11892711]\n",
      "20/01/16 13:17:19 INFO PlinkFileFormat: Reading variants [12742190, 13591669]\n",
      "20/01/16 13:17:27 INFO PlinkFileFormat: Reading variants [25484379, 52671826]\n",
      "20/01/16 13:17:27 ERROR Executor: Exception in task 30.0 in stage 0.0 (TID 30)\n",
      "java.io.EOFException: Cannot seek to a negative offset\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/16 13:17:27 WARN TaskSetManager: Lost task 30.0 in stage 0.0 (TID 30, localhost, executor driver): java.io.EOFException: Cannot seek to a negative offset\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/01/16 13:17:27 ERROR TaskSetManager: Task 30 in stage 0.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 0.0 failed 1 times, most recent failure: Lost task 30.0 in stage 0.0 (TID 30, localhost, executor driver): java.io.EOFException: Cannot seek to a negative offset\n\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\u001b[39m\n  org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1889\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1877\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1876\u001b[39m)\n  scala.collection.mutable.ResizableArray$class.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m59\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m48\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1876\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m257\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2110\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2059\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2048\u001b[39m)\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m737\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2061\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2082\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2101\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2126\u001b[39m)\n  org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(\u001b[32mRDD.scala\u001b[39m:\u001b[32m945\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m112\u001b[39m)\n  org.apache.spark.rdd.RDD.withScope(\u001b[32mRDD.scala\u001b[39m:\u001b[32m363\u001b[39m)\n  org.apache.spark.rdd.RDD.collect(\u001b[32mRDD.scala\u001b[39m:\u001b[32m944\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeCollect(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m299\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$count$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2836\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$count$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2835\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$52.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3370\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3369\u001b[39m)\n  org.apache.spark.sql.Dataset.count(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2835\u001b[39m)\n  ammonite.$sess.cmd1$Helper.<init>(\u001b[32mcmd1.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd1$.<init>(\u001b[32mcmd1.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd1$.<clinit>(\u001b[32mcmd1.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31mjava.io.EOFException: Cannot seek to a negative offset\u001b[39m\n  org.apache.hadoop.fs.FSInputChecker.seek(\u001b[32mFSInputChecker.java\u001b[39m:\u001b[32m399\u001b[39m)\n  org.apache.hadoop.fs.FSDataInputStream.seek(\u001b[32mFSDataInputStream.java\u001b[39m:\u001b[32m62\u001b[39m)\n  org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m330\u001b[39m)\n  io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(\u001b[32mPlinkFileFormat.scala\u001b[39m:\u001b[32m118\u001b[39m)\n  io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(\u001b[32mPlinkFileFormat.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(\u001b[32mFileFormat.scala\u001b[39m:\u001b[32m148\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(\u001b[32mFileFormat.scala\u001b[39m:\u001b[32m132\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m124\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m177\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m101\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m636\u001b[39m)\n  scala.collection.Iterator$$anon$11.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m409\u001b[39m)\n  org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(\u001b[32mBypassMergeSortShuffleWriter.java\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m99\u001b[39m)\n  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m123\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m408\u001b[39m)\n  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1360\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m414\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1149\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m624\u001b[39m)\n  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m748\u001b[39m)"
     ]
    }
   ],
   "source": [
    "ss.read.format(\"plink\").load(data_dir / \"ALL.2of4intersection.20100804.genotypes_no_missing_IDs.bed\" toString).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contigName: string (nullable = true)\n",
      " |-- names: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- position: double (nullable = true)\n",
      " |-- start: long (nullable = true)\n",
      " |-- end: long (nullable = true)\n",
      " |-- referenceAllele: string (nullable = true)\n",
      " |-- alternateAlleles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- genotypes: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- sampleId: string (nullable = true)\n",
      " |    |    |-- calls: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = ss.read.format(\"plink\").load(data_dir / \"ALL.2of4intersection.20100804.genotypes.bed\" toString)\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(size($\"genotypes\")).count.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(data_dir / \"ALL.2of4intersection.20100804.genotypes.parquet\" toString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- variantId: string (nullable = true)\n",
      " |-- sampleId: string (nullable = true)\n",
      " |-- state: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mdfc\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// See https://glow.readthedocs.io/en/latest/etl/utility-functions.html\n",
    "def dfc = df\n",
    "    .selectExpr(\"*\", \"genotype_states(genotypes) as state\")\n",
    "    .withColumn(\"variantId\", $\"names\"(0))\n",
    "    .withColumn(\"sampleId\", $\"genotypes.sampleId\")\n",
    "    .selectExpr(\n",
    "        \"variantId\", \n",
    "        \"explode(arrays_zip(sampleId, state)) as genotypes\"\n",
    "    )\n",
    "    .select(\"variantId\", \"genotypes.*\")\n",
    "dfc.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/01/16 12:41:35 INFO PlinkFileFormat$: hlsUsage:[plinkRead,{\"includeSampleIds\":true,\"mergeFidIid\":true}]\n",
      "20/01/16 12:41:45 INFO PlinkFileFormat: Reading variants [0, 849480]\n",
      "20/01/16 12:41:45 INFO PlinkFileFormat: Reading variants [849480, 1698959]\n",
      "20/01/16 12:41:45 INFO PlinkFileFormat: Reading variants [1698959, 2548438]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [2548438, 3397918]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [3397918, 4247397]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [6795835, 7645314]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [5096876, 5946356]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [5946356, 6795835]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [4247397, 5096876]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [7645314, 8494793]\n",
      "20/01/16 12:41:48 INFO PlinkFileFormat: Reading variants [11892711, 12742190]\n",
      "20/01/16 12:41:48 INFO PlinkFileFormat: Reading variants [9344273, 10193752]\n",
      "20/01/16 12:41:48 INFO PlinkFileFormat: Reading variants [11043231, 11892711]\n",
      "20/01/16 12:41:48 INFO PlinkFileFormat: Reading variants [8494793, 9344273]\n",
      "20/01/16 12:41:48 INFO PlinkFileFormat: Reading variants [10193752, 11043231]\n",
      "20/01/16 12:41:49 INFO PlinkFileFormat: Reading variants [12742190, 13591669]\n",
      "20/01/16 12:53:56 INFO PlinkFileFormat: Reading variants [25484379, 52671826]\n",
      "20/01/16 12:53:57 ERROR Executor: Exception in task 30.0 in stage 0.0 (TID 30)\n",
      "java.io.EOFException: Cannot seek to a negative offset\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/16 12:53:57 WARN TaskSetManager: Lost task 30.0 in stage 0.0 (TID 30, localhost, executor driver): java.io.EOFException: Cannot seek to a negative offset\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/01/16 12:53:57 ERROR TaskSetManager: Task 30 in stage 0.0 failed 1 times; aborting job\n",
      "20/01/16 12:53:57 WARN TaskSetManager: Lost task 8.0 in stage 0.0 (TID 8, localhost, executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 0.0 failed 1 times, most recent failure: Lost task 30.0 in stage 0.0 (TID 30, localhost, executor driver): java.io.EOFException: Cannot seek to a negative offset\n\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\u001b[39m\n  org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1889\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1877\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1876\u001b[39m)\n  scala.collection.mutable.ResizableArray$class.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m59\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m48\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1876\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m257\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2110\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2059\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2048\u001b[39m)\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m737\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2061\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2082\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2101\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeTake(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m365\u001b[39m)\n  org.apache.spark.sql.execution.CollectLimitExec.executeCollect(\u001b[32mlimit.scala\u001b[39m:\u001b[32m38\u001b[39m)\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3389\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$52.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3370\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3369\u001b[39m)\n  org.apache.spark.sql.Dataset.head(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset.take(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2764\u001b[39m)\n  org.apache.spark.sql.Dataset.getRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m254\u001b[39m)\n  org.apache.spark.sql.Dataset.showString(\u001b[32mDataset.scala\u001b[39m:\u001b[32m291\u001b[39m)\n  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m751\u001b[39m)\n  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m710\u001b[39m)\n  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m719\u001b[39m)\n  ammonite.$sess.cmd5$Helper.<init>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd5$.<init>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd5$.<clinit>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31mjava.io.EOFException: Cannot seek to a negative offset\u001b[39m\n  org.apache.hadoop.fs.FSInputChecker.seek(\u001b[32mFSInputChecker.java\u001b[39m:\u001b[32m399\u001b[39m)\n  org.apache.hadoop.fs.FSDataInputStream.seek(\u001b[32mFSDataInputStream.java\u001b[39m:\u001b[32m62\u001b[39m)\n  org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m330\u001b[39m)\n  io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(\u001b[32mPlinkFileFormat.scala\u001b[39m:\u001b[32m118\u001b[39m)\n  io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(\u001b[32mPlinkFileFormat.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(\u001b[32mFileFormat.scala\u001b[39m:\u001b[32m148\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(\u001b[32mFileFormat.scala\u001b[39m:\u001b[32m132\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m124\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m177\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m101\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m636\u001b[39m)\n  scala.collection.Iterator$$anon$12.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m440\u001b[39m)\n  scala.collection.Iterator$JoinIterator.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m212\u001b[39m)\n  scala.collection.Iterator$$anon$11.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m409\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m636\u001b[39m)\n  scala.collection.Iterator$$anon$11.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m409\u001b[39m)\n  org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(\u001b[32mBypassMergeSortShuffleWriter.java\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m99\u001b[39m)\n  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m123\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m408\u001b[39m)\n  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1360\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m414\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1149\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m624\u001b[39m)\n  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m748\u001b[39m)"
     ]
    }
   ],
   "source": [
    "// Frequency of call types showing that homozygous reference is most common by far\n",
    "dfc.groupBy(\"state\").count.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
