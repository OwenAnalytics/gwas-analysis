{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/eczech/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/eczech/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/01/17 22:00:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$           , spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$           , paths._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$          , glow._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mio.projectglow.Glow\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[36mss\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@1f020a3d\n",
       "\u001b[32mimport \u001b[39m\u001b[36mss.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mdata_dir\u001b[39m: \u001b[32mbetter\u001b[39m.\u001b[32mfiles\u001b[39m.\u001b[32mFile\u001b[39m = /home/eczech/data/gwas/tutorial/2_PS_GWAS"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.^.init.spark, spark._\n",
    "import $file.^.init.paths, paths._\n",
    "import $file.^.init.glow, glow._\n",
    "import io.projectglow.Glow\n",
    "import org.apache.spark.sql.functions._\n",
    "val ss = getLocalSparkSession()\n",
    "import ss.implicits._\n",
    "Glow.register(ss)\n",
    "val data_dir = GWAS_TUTORIAL_DATA_DIR / \"2_PS_GWAS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2020-01-17 22:01:14\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "20/01/17 22:01:16 INFO VCFFileFormat: hlsUsage:[vcfRead,{\"useFilterParser\":true,\"splitToBiallelic\":false,\"flattenInfoFields\":true,\"useTabixIndex\":true,\"includeSampleIds\":true}]\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 13.0 in stage 1.0 (TID 29)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 15.0 in stage 1.0 (TID 31)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 9.0 in stage 1.0 (TID 25)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 5.0 in stage 1.0 (TID 21)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 6.0 in stage 1.0 (TID 22)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 8.0 in stage 1.0 (TID 24)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 12.0 in stage 1.0 (TID 28)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 10.0 in stage 1.0 (TID 26)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 11.0 in stage 1.0 (TID 27)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 7.0 in stage 1.0 (TID 23)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 2.0 in stage 1.0 (TID 18)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 17)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 14.0 in stage 1.0 (TID 30)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 4.0 in stage 1.0 (TID 20)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 3.0 in stage 1.0 (TID 19)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 WARN TaskSetManager: Lost task 10.0 in stage 1.0 (TID 26, localhost, executor driver): java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/01/17 22:01:17 ERROR TaskSetManager: Task 10 in stage 1.0 failed 1 times; aborting job\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "WARNING\t2020-01-17 22:01:17\tAsciiLineReader\tCreating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 16.0 in stage 1.0 (TID 32)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 19.0 in stage 1.0 (TID 35)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 18.0 in stage 1.0 (TID 34)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 23.0 in stage 1.0 (TID 39)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 22.0 in stage 1.0 (TID 38)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 17.0 in stage 1.0 (TID 33)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 21.0 in stage 1.0 (TID 37)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 20.0 in stage 1.0 (TID 36)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR FileFormatWriter: Aborting job 127b9b9e-ed62-439d-9d4c-923e8739e6b2.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 1.0 failed 1 times, most recent failure: Lost task 10.0 in stage 1.0 (TID 26, localhost, executor driver): java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n",
      "\tat ammonite.$sess.cmd1$Helper.<init>(cmd1.sc:2)\n",
      "\tat ammonite.$sess.cmd1$.<init>(cmd1.sc:7)\n",
      "\tat ammonite.$sess.cmd1$.<clinit>(cmd1.sc)\n",
      "\tat ammonite.$sess.cmd1.$main(cmd1.sc)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1$$anonfun$evalMain$1.apply(Evaluator.scala:112)\n",
      "\tat ammonite.util.Util$.withContextClassloader(Util.scala:16)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.evalMain(Evaluator.scala:94)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1$$anonfun$processLine$1$$anonfun$apply$6.apply(Evaluator.scala:131)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1$$anonfun$processLine$1$$anonfun$apply$6.apply(Evaluator.scala:125)\n",
      "\tat ammonite.util.Catching.map(Res.scala:117)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1$$anonfun$processLine$1.apply(Evaluator.scala:125)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1$$anonfun$processLine$1.apply(Evaluator.scala:124)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.processLine(Evaluator.scala:124)\n",
      "\tat ammonite.interp.Interpreter$$anonfun$evaluateLine$2$$anonfun$apply$15.apply(Interpreter.scala:287)\n",
      "\tat ammonite.interp.Interpreter$$anonfun$evaluateLine$2$$anonfun$apply$15.apply(Interpreter.scala:281)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter$$anonfun$evaluateLine$2.apply(Interpreter.scala:281)\n",
      "\tat ammonite.interp.Interpreter$$anonfun$evaluateLine$2.apply(Interpreter.scala:280)\n",
      "\tat ammonite.util.Catching.flatMap(Res.scala:115)\n",
      "\tat ammonite.interp.Interpreter.evaluateLine(Interpreter.scala:280)\n",
      "\tat ammonite.interp.Interpreter$$anonfun$processLine$2$$anonfun$apply$9$$anonfun$apply$11.apply(Interpreter.scala:263)\n",
      "\tat ammonite.interp.Interpreter$$anonfun$processLine$2$$anonfun$apply$9$$anonfun$apply$11.apply(Interpreter.scala:251)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter$$anonfun$processLine$2$$anonfun$apply$9.apply(Interpreter.scala:251)\n",
      "\tat ammonite.interp.Interpreter$$anonfun$processLine$2$$anonfun$apply$9.apply(Interpreter.scala:244)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter$$anonfun$processLine$2.apply(Interpreter.scala:244)\n",
      "\tat ammonite.interp.Interpreter$$anonfun$processLine$2.apply(Interpreter.scala:243)\n",
      "\tat ammonite.util.Catching.flatMap(Res.scala:115)\n",
      "\tat ammonite.interp.Interpreter.processLine(Interpreter.scala:243)\n",
      "\tat almond.Execute$$anonfun$ammResult$1$$anonfun$apply$12$$anonfun$apply$13$$anonfun$apply$14$$anonfun$apply$15$$anonfun$apply$16.apply(Execute.scala:227)\n",
      "\tat almond.Execute$$anonfun$ammResult$1$$anonfun$apply$12$$anonfun$apply$13$$anonfun$apply$14$$anonfun$apply$15$$anonfun$apply$16.apply(Execute.scala:223)\n",
      "\tat almond.internals.CaptureImpl$$anonfun$apply$1$$anonfun$apply$2.apply(CaptureImpl.scala:53)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
      "\tat scala.Console$.withErr(Console.scala:80)\n",
      "\tat almond.internals.CaptureImpl$$anonfun$apply$1.apply(CaptureImpl.scala:45)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
      "\tat scala.Console$.withOut(Console.scala:53)\n",
      "\tat almond.internals.CaptureImpl.apply(CaptureImpl.scala:44)\n",
      "\tat almond.Execute.almond$Execute$$capturingOutput(Execute.scala:165)\n",
      "\tat almond.Execute$$anonfun$ammResult$1$$anonfun$apply$12$$anonfun$apply$13$$anonfun$apply$14$$anonfun$apply$15.apply(Execute.scala:223)\n",
      "\tat almond.Execute$$anonfun$ammResult$1$$anonfun$apply$12$$anonfun$apply$13$$anonfun$apply$14$$anonfun$apply$15.apply(Execute.scala:223)\n",
      "\tat almond.Execute$$anonfun$almond$Execute$$withClientStdin$1.apply(Execute.scala:145)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
      "\tat scala.Console$.withIn(Console.scala:124)\n",
      "\tat almond.Execute.almond$Execute$$withClientStdin(Execute.scala:141)\n",
      "\tat almond.Execute$$anonfun$ammResult$1$$anonfun$apply$12$$anonfun$apply$13$$anonfun$apply$14.apply(Execute.scala:222)\n",
      "\tat almond.Execute$$anonfun$ammResult$1$$anonfun$apply$12$$anonfun$apply$13$$anonfun$apply$14.apply(Execute.scala:222)\n",
      "\tat almond.Execute.almond$Execute$$withInputManager(Execute.scala:133)\n",
      "\tat almond.Execute$$anonfun$ammResult$1$$anonfun$apply$12$$anonfun$apply$13.apply(Execute.scala:221)\n",
      "\tat almond.Execute$$anonfun$ammResult$1$$anonfun$apply$12$$anonfun$apply$13.apply(Execute.scala:221)\n",
      "\tat ammonite.repl.Signaller.apply(Signaller.scala:28)\n",
      "\tat almond.Execute.almond$Execute$$interruptible(Execute.scala:181)\n",
      "\tat almond.Execute$$anonfun$ammResult$1$$anonfun$apply$12.apply(Execute.scala:220)\n",
      "\tat almond.Execute$$anonfun$ammResult$1$$anonfun$apply$12.apply(Execute.scala:212)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat almond.Execute$$anonfun$ammResult$1.apply(Execute.scala:212)\n",
      "\tat almond.Execute$$anonfun$ammResult$1.apply(Execute.scala:212)\n",
      "\tat almond.Execute.withOutputHandler(Execute.scala:156)\n",
      "\tat almond.Execute.ammResult(Execute.scala:210)\n",
      "\tat almond.Execute.apply(Execute.scala:309)\n",
      "\tat almond.ScalaInterpreter.execute(ScalaInterpreter.scala:116)\n",
      "\tat almond.interpreter.InterpreterToIOInterpreter$$anonfun$execute$1$$anonfun$apply$3.apply(InterpreterToIOInterpreter.scala:69)\n",
      "\tat almond.interpreter.InterpreterToIOInterpreter$$anonfun$execute$1$$anonfun$apply$3.apply(InterpreterToIOInterpreter.scala:66)\n",
      "\tat cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316)\n",
      "\tat cats.effect.internals.IOShift$Tick.run(IOShift.scala:36)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\t... 3 more\n",
      "20/01/17 22:01:17 ERROR Executor: Exception in task 24.0 in stage 1.0 (TID 40)\n",
      "java.lang.NumberFormatException: For input string: \".\"\n",
      "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
      "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
      "\tat java.lang.Double.parseDouble(Double.java:538)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n",
      "\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n",
      "\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n",
      "\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n",
      "\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n",
      "\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n",
      "\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n",
      "\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n",
      "\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/17 22:01:17 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:149)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:100)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted.\u001b[39m\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m198\u001b[39m)\n  org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(\u001b[32mInsertIntoHadoopFsRelationCommand.scala\u001b[39m:\u001b[32m159\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m102\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m122\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.toRdd(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m271\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m229\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.parquet(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m566\u001b[39m)\n  ammonite.$sess.cmd1$Helper.<init>(\u001b[32mcmd1.sc\u001b[39m:\u001b[32m2\u001b[39m)\n  ammonite.$sess.cmd1$.<init>(\u001b[32mcmd1.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd1$.<clinit>(\u001b[32mcmd1.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 1.0 failed 1 times, most recent failure: Lost task 10.0 in stage 1.0 (TID 26, localhost, executor driver): java.lang.NumberFormatException: For input string: \".\"\n\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.lang.Double.parseDouble(Double.java:538)\n\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(GenotypeLikelihoods.java:264)\n\tat htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(GenotypeLikelihoods.java:90)\n\tat htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:741)\n\tat htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132)\n\tat htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158)\n\tat htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148)\n\tat htsjdk.variant.variantcontext.GenotypesContext.get(GenotypesContext.java:417)\n\tat htsjdk.variant.variantcontext.VariantContext.getGenotype(VariantContext.java:1013)\n\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:85)\n\tat io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(VariantContextToInternalRowConverter.scala:80)\n\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:41)\n\tat io.projectglow.sql.util.RowConverter.apply(RowConverter.scala:34)\n\tat io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(VariantContextToInternalRowConverter.scala:140)\n\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:536)\n\tat io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(VCFFileFormat.scala:535)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:104)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\u001b[39m\n  org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1889\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1877\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1876\u001b[39m)\n  scala.collection.mutable.ResizableArray$class.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m59\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m48\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1876\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m257\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2110\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2059\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2048\u001b[39m)\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m737\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2061\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m167\u001b[39m)\n  org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(\u001b[32mInsertIntoHadoopFsRelationCommand.scala\u001b[39m:\u001b[32m159\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m102\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m122\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.toRdd(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m271\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m229\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.parquet(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m566\u001b[39m)\n  ammonite.$sess.cmd1$Helper.<init>(\u001b[32mcmd1.sc\u001b[39m:\u001b[32m2\u001b[39m)\n  ammonite.$sess.cmd1$.<init>(\u001b[32mcmd1.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd1$.<clinit>(\u001b[32mcmd1.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31mjava.lang.NumberFormatException: For input string: \".\"\u001b[39m\n  sun.misc.FloatingDecimal.readJavaFormatString(\u001b[32mFloatingDecimal.java\u001b[39m:\u001b[32m2043\u001b[39m)\n  sun.misc.FloatingDecimal.parseDouble(\u001b[32mFloatingDecimal.java\u001b[39m:\u001b[32m110\u001b[39m)\n  java.lang.Double.parseDouble(\u001b[32mDouble.java\u001b[39m:\u001b[32m538\u001b[39m)\n  htsjdk.variant.variantcontext.GenotypeLikelihoods.parseDeprecatedGLString(\u001b[32mGenotypeLikelihoods.java\u001b[39m:\u001b[32m264\u001b[39m)\n  htsjdk.variant.variantcontext.GenotypeLikelihoods.fromGLField(\u001b[32mGenotypeLikelihoods.java\u001b[39m:\u001b[32m90\u001b[39m)\n  htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(\u001b[32mAbstractVCFCodec.java\u001b[39m:\u001b[32m741\u001b[39m)\n  htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(\u001b[32mAbstractVCFCodec.java\u001b[39m:\u001b[32m132\u001b[39m)\n  htsjdk.variant.variantcontext.LazyGenotypesContext.decode(\u001b[32mLazyGenotypesContext.java\u001b[39m:\u001b[32m158\u001b[39m)\n  htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(\u001b[32mLazyGenotypesContext.java\u001b[39m:\u001b[32m148\u001b[39m)\n  htsjdk.variant.variantcontext.GenotypesContext.get(\u001b[32mGenotypesContext.java\u001b[39m:\u001b[32m417\u001b[39m)\n  htsjdk.variant.variantcontext.VariantContext.getGenotype(\u001b[32mVariantContext.java\u001b[39m:\u001b[32m1013\u001b[39m)\n  io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(\u001b[32mVariantContextToInternalRowConverter.scala\u001b[39m:\u001b[32m85\u001b[39m)\n  io.projectglow.vcf.VariantContextToInternalRowConverter$$anonfun$1$$anonfun$13.apply(\u001b[32mVariantContextToInternalRowConverter.scala\u001b[39m:\u001b[32m80\u001b[39m)\n  io.projectglow.sql.util.RowConverter.apply(\u001b[32mRowConverter.scala\u001b[39m:\u001b[32m41\u001b[39m)\n  io.projectglow.sql.util.RowConverter.apply(\u001b[32mRowConverter.scala\u001b[39m:\u001b[32m34\u001b[39m)\n  io.projectglow.vcf.VariantContextToInternalRowConverter.convertRow(\u001b[32mVariantContextToInternalRowConverter.scala\u001b[39m:\u001b[32m140\u001b[39m)\n  io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(\u001b[32mVCFFileFormat.scala\u001b[39m:\u001b[32m536\u001b[39m)\n  io.projectglow.vcf.SchemaDelegate$FlattenedInfoDelegate$$anonfun$toRows$3.apply(\u001b[32mVCFFileFormat.scala\u001b[39m:\u001b[32m535\u001b[39m)\n  scala.collection.Iterator$$anon$11.next(\u001b[32mIterator.scala\u001b[39m:\u001b[32m410\u001b[39m)\n  scala.collection.Iterator$$anon$11.next(\u001b[32mIterator.scala\u001b[39m:\u001b[32m410\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m636\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m232\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m170\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m169\u001b[39m)\n  org.apache.spark.scheduler.ResultTask.runTask(\u001b[32mResultTask.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m123\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m408\u001b[39m)\n  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1360\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m414\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1149\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m624\u001b[39m)\n  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m748\u001b[39m)"
     ]
    }
   ],
   "source": [
    "ss.read.format(\"vcf\").load(data_dir / \"ALL.2of4intersection.20100804.genotypes.vcf.gz\" toString)\n",
    "    .write.parquet(data_dir / \"ALL.2of4intersection.20100804.genotypes.parquet\" toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/01/16 14:55:40 INFO PlinkFileFormat$: hlsUsage:[plinkRead,{\"includeSampleIds\":true,\"mergeFidIid\":true}]\n",
      "20/01/16 14:55:42 INFO PlinkFileFormat: Reading variants [1094036, 1458715]\n",
      "20/01/16 14:55:43 INFO PlinkFileFormat: Reading variants [364679, 729358]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [0, 364679]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [729358, 1094036]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [1823393, 2188072]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [1458715, 1823393]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [3646786, 4011464]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [3282107, 3646786]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [2188072, 2552750]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [2917429, 3282107]\n",
      "20/01/16 14:55:44 INFO PlinkFileFormat: Reading variants [4011464, 4376143]\n",
      "20/01/16 14:55:45 INFO PlinkFileFormat: Reading variants [5470178, 5808310]\n",
      "20/01/16 14:55:45 INFO PlinkFileFormat: Reading variants [2552750, 2917429]\n",
      "20/01/16 14:55:45 INFO PlinkFileFormat: Reading variants [4740821, 5105500]\n",
      "20/01/16 14:55:45 INFO PlinkFileFormat: Reading variants [4376143, 4740821]\n",
      "20/01/16 14:55:45 INFO PlinkFileFormat: Reading variants [5105500, 5470178]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres12\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m5808310L\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.read.format(\"plink\").load(data_dir / \"1kG_MDS5.bed\" toString).drop(\"genotypes\").count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/01/16 13:24:03 INFO PlinkFileFormat$: hlsUsage:[plinkRead,{\"includeSampleIds\":true,\"mergeFidIid\":true}]\n",
      "20/01/16 13:24:06 INFO PlinkFileFormat: Reading variants [0, 364679]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [364679, 729358]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [729358, 1094036]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [1458715, 1823393]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [1094036, 1458715]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [1823393, 2188072]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [2552750, 2917429]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [2188072, 2552750]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [2917429, 3282107]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [4011464, 4376143]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [3646786, 4011464]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [3282107, 3646786]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [4376143, 4740821]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [5105500, 5470178]\n",
      "20/01/16 13:24:07 INFO PlinkFileFormat: Reading variants [4740821, 5105500]\n",
      "20/01/16 13:24:08 INFO PlinkFileFormat: Reading variants [5470178, 5808310]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres2\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m5808310L\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.read.format(\"plink\").load(data_dir / \"1kG_MDS5.bed\" toString).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/01/16 13:17:05 INFO PlinkFileFormat$: hlsUsage:[plinkRead,{\"includeSampleIds\":true,\"mergeFidIid\":true}]\n",
      "20/01/16 13:17:12 INFO PlinkFileFormat: Reading variants [0, 849480]\n",
      "20/01/16 13:17:12 INFO PlinkFileFormat: Reading variants [849480, 1698959]\n",
      "20/01/16 13:17:13 INFO PlinkFileFormat: Reading variants [1698959, 2548438]\n",
      "20/01/16 13:17:17 INFO PlinkFileFormat: Reading variants [2548438, 3397918]\n",
      "20/01/16 13:17:17 INFO PlinkFileFormat: Reading variants [4247397, 5096876]\n",
      "20/01/16 13:17:17 INFO PlinkFileFormat: Reading variants [3397918, 4247397]\n",
      "20/01/16 13:17:17 INFO PlinkFileFormat: Reading variants [5096876, 5946356]\n",
      "20/01/16 13:17:18 INFO PlinkFileFormat: Reading variants [5946356, 6795835]\n",
      "20/01/16 13:17:18 INFO PlinkFileFormat: Reading variants [7645314, 8494793]\n",
      "20/01/16 13:17:18 INFO PlinkFileFormat: Reading variants [6795835, 7645314]\n",
      "20/01/16 13:17:18 INFO PlinkFileFormat: Reading variants [8494793, 9344273]\n",
      "20/01/16 13:17:18 INFO PlinkFileFormat: Reading variants [9344273, 10193752]\n",
      "20/01/16 13:17:19 INFO PlinkFileFormat: Reading variants [10193752, 11043231]\n",
      "20/01/16 13:17:19 INFO PlinkFileFormat: Reading variants [11892711, 12742190]\n",
      "20/01/16 13:17:19 INFO PlinkFileFormat: Reading variants [11043231, 11892711]\n",
      "20/01/16 13:17:19 INFO PlinkFileFormat: Reading variants [12742190, 13591669]\n",
      "20/01/16 13:17:27 INFO PlinkFileFormat: Reading variants [25484379, 52671826]\n",
      "20/01/16 13:17:27 ERROR Executor: Exception in task 30.0 in stage 0.0 (TID 30)\n",
      "java.io.EOFException: Cannot seek to a negative offset\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/16 13:17:27 WARN TaskSetManager: Lost task 30.0 in stage 0.0 (TID 30, localhost, executor driver): java.io.EOFException: Cannot seek to a negative offset\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/01/16 13:17:27 ERROR TaskSetManager: Task 30 in stage 0.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 0.0 failed 1 times, most recent failure: Lost task 30.0 in stage 0.0 (TID 30, localhost, executor driver): java.io.EOFException: Cannot seek to a negative offset\n\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\u001b[39m\n  org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1889\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1877\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1876\u001b[39m)\n  scala.collection.mutable.ResizableArray$class.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m59\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m48\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1876\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m257\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2110\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2059\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2048\u001b[39m)\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m737\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2061\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2082\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2101\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2126\u001b[39m)\n  org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(\u001b[32mRDD.scala\u001b[39m:\u001b[32m945\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m112\u001b[39m)\n  org.apache.spark.rdd.RDD.withScope(\u001b[32mRDD.scala\u001b[39m:\u001b[32m363\u001b[39m)\n  org.apache.spark.rdd.RDD.collect(\u001b[32mRDD.scala\u001b[39m:\u001b[32m944\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeCollect(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m299\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$count$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2836\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$count$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2835\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$52.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3370\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3369\u001b[39m)\n  org.apache.spark.sql.Dataset.count(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2835\u001b[39m)\n  ammonite.$sess.cmd1$Helper.<init>(\u001b[32mcmd1.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd1$.<init>(\u001b[32mcmd1.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd1$.<clinit>(\u001b[32mcmd1.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31mjava.io.EOFException: Cannot seek to a negative offset\u001b[39m\n  org.apache.hadoop.fs.FSInputChecker.seek(\u001b[32mFSInputChecker.java\u001b[39m:\u001b[32m399\u001b[39m)\n  org.apache.hadoop.fs.FSDataInputStream.seek(\u001b[32mFSDataInputStream.java\u001b[39m:\u001b[32m62\u001b[39m)\n  org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m330\u001b[39m)\n  io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(\u001b[32mPlinkFileFormat.scala\u001b[39m:\u001b[32m118\u001b[39m)\n  io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(\u001b[32mPlinkFileFormat.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(\u001b[32mFileFormat.scala\u001b[39m:\u001b[32m148\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(\u001b[32mFileFormat.scala\u001b[39m:\u001b[32m132\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m124\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m177\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m101\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m636\u001b[39m)\n  scala.collection.Iterator$$anon$11.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m409\u001b[39m)\n  org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(\u001b[32mBypassMergeSortShuffleWriter.java\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m99\u001b[39m)\n  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m123\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m408\u001b[39m)\n  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1360\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m414\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1149\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m624\u001b[39m)\n  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m748\u001b[39m)"
     ]
    }
   ],
   "source": [
    "ss.read.format(\"plink\").load(data_dir / \"ALL.2of4intersection.20100804.genotypes_no_missing_IDs.bed\" toString).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contigName: string (nullable = true)\n",
      " |-- names: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- position: double (nullable = true)\n",
      " |-- start: long (nullable = true)\n",
      " |-- end: long (nullable = true)\n",
      " |-- referenceAllele: string (nullable = true)\n",
      " |-- alternateAlleles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- genotypes: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- sampleId: string (nullable = true)\n",
      " |    |    |-- calls: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = ss.read.format(\"plink\").load(data_dir / \"ALL.2of4intersection.20100804.genotypes.bed\" toString)\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/01/16 13:14:54 INFO PlinkFileFormat$: hlsUsage:[plinkRead,{\"includeSampleIds\":true,\"mergeFidIid\":true}]\n",
      "20/01/16 13:15:02 INFO PlinkFileFormat: Reading variants [0, 849480]\n",
      "20/01/16 13:15:02 INFO PlinkFileFormat: Reading variants [849480, 1698959]\n",
      "20/01/16 13:15:02 INFO PlinkFileFormat: Reading variants [1698959, 2548438]\n",
      "20/01/16 13:15:06 INFO PlinkFileFormat: Reading variants [2548438, 3397918]\n",
      "20/01/16 13:15:06 INFO PlinkFileFormat: Reading variants [3397918, 4247397]\n",
      "20/01/16 13:15:06 INFO PlinkFileFormat: Reading variants [4247397, 5096876]\n",
      "20/01/16 13:15:07 INFO PlinkFileFormat: Reading variants [5096876, 5946356]\n",
      "20/01/16 13:15:07 INFO PlinkFileFormat: Reading variants [5946356, 6795835]\n",
      "20/01/16 13:15:07 INFO PlinkFileFormat: Reading variants [6795835, 7645314]\n",
      "20/01/16 13:15:07 INFO PlinkFileFormat: Reading variants [8494793, 9344273]\n",
      "20/01/16 13:15:07 INFO PlinkFileFormat: Reading variants [9344273, 10193752]\n",
      "20/01/16 13:15:07 INFO PlinkFileFormat: Reading variants [7645314, 8494793]\n",
      "20/01/16 13:15:07 INFO PlinkFileFormat: Reading variants [10193752, 11043231]\n",
      "20/01/16 13:15:08 INFO PlinkFileFormat: Reading variants [11043231, 11892711]\n",
      "20/01/16 13:15:08 INFO PlinkFileFormat: Reading variants [11892711, 12742190]\n",
      "20/01/16 13:15:08 INFO PlinkFileFormat: Reading variants [12742190, 13591669]\n",
      "20/01/16 13:15:17 INFO PlinkFileFormat: Reading variants [25484379, 52671826]\n",
      "20/01/16 13:15:17 ERROR Executor: Exception in task 30.0 in stage 0.0 (TID 30)\n",
      "java.io.EOFException: Cannot seek to a negative offset\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/16 13:15:17 WARN TaskSetManager: Lost task 30.0 in stage 0.0 (TID 30, localhost, executor driver): java.io.EOFException: Cannot seek to a negative offset\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/01/16 13:15:17 ERROR TaskSetManager: Task 30 in stage 0.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 0.0 failed 1 times, most recent failure: Lost task 30.0 in stage 0.0 (TID 30, localhost, executor driver): java.io.EOFException: Cannot seek to a negative offset\n\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\u001b[39m\n  org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1889\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1877\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1876\u001b[39m)\n  scala.collection.mutable.ResizableArray$class.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m59\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m48\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1876\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m257\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2110\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2059\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2048\u001b[39m)\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m737\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2061\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2082\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2101\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2126\u001b[39m)\n  org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(\u001b[32mRDD.scala\u001b[39m:\u001b[32m945\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m112\u001b[39m)\n  org.apache.spark.rdd.RDD.withScope(\u001b[32mRDD.scala\u001b[39m:\u001b[32m363\u001b[39m)\n  org.apache.spark.rdd.RDD.collect(\u001b[32mRDD.scala\u001b[39m:\u001b[32m944\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeCollect(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m299\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$count$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2836\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$count$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2835\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$52.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3370\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3369\u001b[39m)\n  org.apache.spark.sql.Dataset.count(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2835\u001b[39m)\n  ammonite.$sess.cmd3$Helper.<init>(\u001b[32mcmd3.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd3$.<init>(\u001b[32mcmd3.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd3$.<clinit>(\u001b[32mcmd3.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31mjava.io.EOFException: Cannot seek to a negative offset\u001b[39m\n  org.apache.hadoop.fs.FSInputChecker.seek(\u001b[32mFSInputChecker.java\u001b[39m:\u001b[32m399\u001b[39m)\n  org.apache.hadoop.fs.FSDataInputStream.seek(\u001b[32mFSDataInputStream.java\u001b[39m:\u001b[32m62\u001b[39m)\n  org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m330\u001b[39m)\n  io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(\u001b[32mPlinkFileFormat.scala\u001b[39m:\u001b[32m118\u001b[39m)\n  io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(\u001b[32mPlinkFileFormat.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(\u001b[32mFileFormat.scala\u001b[39m:\u001b[32m148\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(\u001b[32mFileFormat.scala\u001b[39m:\u001b[32m132\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m124\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m177\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m101\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m636\u001b[39m)\n  scala.collection.Iterator$$anon$11.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m409\u001b[39m)\n  org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(\u001b[32mBypassMergeSortShuffleWriter.java\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m99\u001b[39m)\n  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m123\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m408\u001b[39m)\n  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1360\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m414\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1149\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m624\u001b[39m)\n  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m748\u001b[39m)"
     ]
    }
   ],
   "source": [
    "df.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(size($\"genotypes\")).count.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/01/16 13:13:22 INFO PlinkFileFormat$: hlsUsage:[plinkRead,{\"includeSampleIds\":true,\"mergeFidIid\":true}]\n",
      "20/01/16 13:13:27 INFO PlinkFileFormat: Reading variants [849480, 1698959]\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkFileFormat: Reading variants [1698959, 2548438]\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkFileFormat: Reading variants [2548438, 3397918]\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkFileFormat: Reading variants [0, 849480]\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:27 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkFileFormat: Reading variants [4247397, 5096876]\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkFileFormat: Reading variants [3397918, 4247397]\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkFileFormat: Reading variants [5096876, 5946356]\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkFileFormat: Reading variants [5946356, 6795835]\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkFileFormat: Reading variants [6795835, 7645314]\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:28 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkFileFormat: Reading variants [7645314, 8494793]\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkFileFormat: Reading variants [8494793, 9344273]\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkFileFormat: Reading variants [9344273, 10193752]\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:29 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkFileFormat: Reading variants [11043231, 11892711]\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkFileFormat: Reading variants [10193752, 11043231]\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkFileFormat: Reading variants [11892711, 12742190]\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkFileFormat: Reading variants [12742190, 13591669]\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(contigName,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(names,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(position,DoubleType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(start,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(end,LongType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(referenceAllele,StringType,true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:13:30 INFO PlinkRowToInternalRowConverter: Column StructField(alternateAlleles,ArrayType(StringType,true),true) cannot be derived from PLINK records. It will be null for each row.\n",
      "20/01/16 13:14:29 ERROR FileFormatWriter: Aborting job 66639cfd-d295-49fe-975b-5f6146e72fb2.\n",
      "java.lang.ThreadDeath\n",
      "\tat java.lang.Thread.stop(Thread.java:853)\n",
      "\tat almond.Execute$$anonfun$almond$Execute$$interruptible$1.apply$mcV$sp(Execute.scala:179)\n",
      "\tat ammonite.repl.Signaller$$anon$1.handle(Signaller.scala:21)\n",
      "\tat sun.misc.Signal$1.run(Signal.java:212)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/16 13:14:29 ERROR FileFormatWriter: Aborting job 66639cfd-d295-49fe-975b-5f6146e72fb2.\n",
      "java.lang.ThreadDeath\n",
      "\tat java.lang.Thread.stop(Thread.java:853)\n",
      "\tat almond.Execute$$anonfun$almond$Execute$$interruptible$1.apply$mcV$sp(Execute.scala:179)\n",
      "\tat ammonite.repl.Signaller$$anon$1.handle(Signaller.scala:21)\n",
      "\tat sun.misc.Signal$1.run(Signal.java:212)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "log4j:ERROR Failed to flush writer,"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mjava.lang.InterruptedException\u001b[39m\n  java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(\u001b[32mAbstractQueuedSynchronizer.java\u001b[39m:\u001b[32m1302\u001b[39m)\n  cats.effect.internals.IOPlatform$$anonfun$unsafeResync$1.apply$mcV$sp(\u001b[32mIOPlatform.scala\u001b[39m:\u001b[32m51\u001b[39m)\n  cats.effect.internals.IOPlatform$$anonfun$unsafeResync$1.apply(\u001b[32mIOPlatform.scala\u001b[39m:\u001b[32m51\u001b[39m)\n  cats.effect.internals.IOPlatform$$anonfun$unsafeResync$1.apply(\u001b[32mIOPlatform.scala\u001b[39m:\u001b[32m51\u001b[39m)\n  scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(\u001b[32mBlockContext.scala\u001b[39m:\u001b[32m53\u001b[39m)\n  scala.concurrent.package$.blocking(\u001b[32mpackage.scala\u001b[39m:\u001b[32m123\u001b[39m)\n  cats.effect.internals.IOPlatform$.unsafeResync(\u001b[32mIOPlatform.scala\u001b[39m:\u001b[32m51\u001b[39m)\n  cats.effect.IO.unsafeRunTimed(\u001b[32mIO.scala\u001b[39m:\u001b[32m325\u001b[39m)\n  cats.effect.IO.unsafeRunSync(\u001b[32mIO.scala\u001b[39m:\u001b[32m240\u001b[39m)\n  almond.interpreter.messagehandlers.InterpreterMessageHandlers$QueueOutputHandler.print(\u001b[32mInterpreterMessageHandlers.scala\u001b[39m:\u001b[32m232\u001b[39m)\n  almond.interpreter.messagehandlers.InterpreterMessageHandlers$QueueOutputHandler.stderr(\u001b[32mInterpreterMessageHandlers.scala\u001b[39m:\u001b[32m237\u001b[39m)\n  almond.Execute$$anonfun$almond$Execute$$capturingOutput$2.apply(\u001b[32mExecute.scala\u001b[39m:\u001b[32m165\u001b[39m)\n  almond.Execute$$anonfun$almond$Execute$$capturingOutput$2.apply(\u001b[32mExecute.scala\u001b[39m:\u001b[32m165\u001b[39m)\n  almond.internals.CaptureImpl$$anonfun$2.apply(\u001b[32mCaptureImpl.scala\u001b[39m:\u001b[32m30\u001b[39m)\n  almond.internals.CaptureImpl$$anonfun$2.apply(\u001b[32mCaptureImpl.scala\u001b[39m:\u001b[32m30\u001b[39m)\n  almond.internals.FunctionOutputStream.flush(\u001b[32mFunctionOutputStream.scala\u001b[39m:\u001b[32m71\u001b[39m)\n  java.io.PrintStream.write(\u001b[32mPrintStream.java\u001b[39m:\u001b[32m482\u001b[39m)\n  sun.nio.cs.StreamEncoder.writeBytes(\u001b[32mStreamEncoder.java\u001b[39m:\u001b[32m221\u001b[39m)\n  sun.nio.cs.StreamEncoder.implFlushBuffer(\u001b[32mStreamEncoder.java\u001b[39m:\u001b[32m291\u001b[39m)\n  sun.nio.cs.StreamEncoder.flushBuffer(\u001b[32mStreamEncoder.java\u001b[39m:\u001b[32m104\u001b[39m)\n  java.io.OutputStreamWriter.flushBuffer(\u001b[32mOutputStreamWriter.java\u001b[39m:\u001b[32m185\u001b[39m)\n  java.io.PrintStream.write(\u001b[32mPrintStream.java\u001b[39m:\u001b[32m527\u001b[39m)\n  java.io.PrintStream.print(\u001b[32mPrintStream.java\u001b[39m:\u001b[32m669\u001b[39m)\n  java.io.PrintStream.println(\u001b[32mPrintStream.java\u001b[39m:\u001b[32m806\u001b[39m)\n  org.apache.log4j.helpers.LogLog.error(\u001b[32mLogLog.java\u001b[39m:\u001b[32m143\u001b[39m)\n  org.apache.log4j.helpers.OnlyOnceErrorHandler.error(\u001b[32mOnlyOnceErrorHandler.java\u001b[39m:\u001b[32m83\u001b[39m)\n  org.apache.log4j.helpers.OnlyOnceErrorHandler.error(\u001b[32mOnlyOnceErrorHandler.java\u001b[39m:\u001b[32m70\u001b[39m)\n  org.apache.log4j.helpers.QuietWriter.flush(\u001b[32mQuietWriter.java\u001b[39m:\u001b[32m61\u001b[39m)\n  org.apache.log4j.WriterAppender.subAppend(\u001b[32mWriterAppender.java\u001b[39m:\u001b[32m324\u001b[39m)\n  org.apache.log4j.WriterAppender.append(\u001b[32mWriterAppender.java\u001b[39m:\u001b[32m162\u001b[39m)\n  org.apache.log4j.AppenderSkeleton.doAppend(\u001b[32mAppenderSkeleton.java\u001b[39m:\u001b[32m251\u001b[39m)\n  org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(\u001b[32mAppenderAttachableImpl.java\u001b[39m:\u001b[32m66\u001b[39m)\n  org.apache.log4j.Category.callAppenders(\u001b[32mCategory.java\u001b[39m:\u001b[32m206\u001b[39m)\n  org.apache.log4j.Category.forcedLog(\u001b[32mCategory.java\u001b[39m:\u001b[32m391\u001b[39m)\n  org.apache.log4j.Category.log(\u001b[32mCategory.java\u001b[39m:\u001b[32m856\u001b[39m)\n  org.slf4j.impl.Log4jLoggerAdapter.error(\u001b[32mLog4jLoggerAdapter.java\u001b[39m:\u001b[32m576\u001b[39m)\n  org.apache.spark.internal.Logging$class.logError(\u001b[32mLogging.scala\u001b[39m:\u001b[32m91\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.logError(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m44\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m196\u001b[39m)\n  org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(\u001b[32mInsertIntoHadoopFsRelationCommand.scala\u001b[39m:\u001b[32m159\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m102\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m122\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.toRdd(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m271\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m229\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.parquet(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m566\u001b[39m)\n  ammonite.$sess.cmd6$Helper.<init>(\u001b[32mcmd6.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd6$.<init>(\u001b[32mcmd6.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd6$.<clinit>(\u001b[32mcmd6.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(data_dir / \"ALL.2of4intersection.20100804.genotypes.parquet\" toString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- variantId: string (nullable = true)\n",
      " |-- sampleId: string (nullable = true)\n",
      " |-- state: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mdfc\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// See https://glow.readthedocs.io/en/latest/etl/utility-functions.html\n",
    "def dfc = df\n",
    "    .selectExpr(\"*\", \"genotype_states(genotypes) as state\")\n",
    "    .withColumn(\"variantId\", $\"names\"(0))\n",
    "    .withColumn(\"sampleId\", $\"genotypes.sampleId\")\n",
    "    .selectExpr(\n",
    "        \"variantId\", \n",
    "        \"explode(arrays_zip(sampleId, state)) as genotypes\"\n",
    "    )\n",
    "    .select(\"variantId\", \"genotypes.*\")\n",
    "dfc.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/01/16 12:41:35 INFO PlinkFileFormat$: hlsUsage:[plinkRead,{\"includeSampleIds\":true,\"mergeFidIid\":true}]\n",
      "20/01/16 12:41:45 INFO PlinkFileFormat: Reading variants [0, 849480]\n",
      "20/01/16 12:41:45 INFO PlinkFileFormat: Reading variants [849480, 1698959]\n",
      "20/01/16 12:41:45 INFO PlinkFileFormat: Reading variants [1698959, 2548438]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [2548438, 3397918]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [3397918, 4247397]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [6795835, 7645314]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [5096876, 5946356]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [5946356, 6795835]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [4247397, 5096876]\n",
      "20/01/16 12:41:46 INFO PlinkFileFormat: Reading variants [7645314, 8494793]\n",
      "20/01/16 12:41:48 INFO PlinkFileFormat: Reading variants [11892711, 12742190]\n",
      "20/01/16 12:41:48 INFO PlinkFileFormat: Reading variants [9344273, 10193752]\n",
      "20/01/16 12:41:48 INFO PlinkFileFormat: Reading variants [11043231, 11892711]\n",
      "20/01/16 12:41:48 INFO PlinkFileFormat: Reading variants [8494793, 9344273]\n",
      "20/01/16 12:41:48 INFO PlinkFileFormat: Reading variants [10193752, 11043231]\n",
      "20/01/16 12:41:49 INFO PlinkFileFormat: Reading variants [12742190, 13591669]\n",
      "20/01/16 12:53:56 INFO PlinkFileFormat: Reading variants [25484379, 52671826]\n",
      "20/01/16 12:53:57 ERROR Executor: Exception in task 30.0 in stage 0.0 (TID 30)\n",
      "java.io.EOFException: Cannot seek to a negative offset\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/01/16 12:53:57 WARN TaskSetManager: Lost task 30.0 in stage 0.0 (TID 30, localhost, executor driver): java.io.EOFException: Cannot seek to a negative offset\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n",
      "\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/01/16 12:53:57 ERROR TaskSetManager: Task 30 in stage 0.0 failed 1 times; aborting job\n",
      "20/01/16 12:53:57 WARN TaskSetManager: Lost task 8.0 in stage 0.0 (TID 8, localhost, executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 0.0 failed 1 times, most recent failure: Lost task 30.0 in stage 0.0 (TID 30, localhost, executor driver): java.io.EOFException: Cannot seek to a negative offset\n\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399)\n\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:330)\n\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:118)\n\tat io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(PlinkFileFormat.scala:90)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\u001b[39m\n  org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1889\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1877\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1876\u001b[39m)\n  scala.collection.mutable.ResizableArray$class.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m59\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m48\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1876\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m257\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2110\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2059\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2048\u001b[39m)\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m737\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2061\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2082\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2101\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeTake(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m365\u001b[39m)\n  org.apache.spark.sql.execution.CollectLimitExec.executeCollect(\u001b[32mlimit.scala\u001b[39m:\u001b[32m38\u001b[39m)\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3389\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$52.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3370\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3369\u001b[39m)\n  org.apache.spark.sql.Dataset.head(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset.take(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2764\u001b[39m)\n  org.apache.spark.sql.Dataset.getRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m254\u001b[39m)\n  org.apache.spark.sql.Dataset.showString(\u001b[32mDataset.scala\u001b[39m:\u001b[32m291\u001b[39m)\n  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m751\u001b[39m)\n  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m710\u001b[39m)\n  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m719\u001b[39m)\n  ammonite.$sess.cmd5$Helper.<init>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd5$.<init>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd5$.<clinit>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31mjava.io.EOFException: Cannot seek to a negative offset\u001b[39m\n  org.apache.hadoop.fs.FSInputChecker.seek(\u001b[32mFSInputChecker.java\u001b[39m:\u001b[32m399\u001b[39m)\n  org.apache.hadoop.fs.FSDataInputStream.seek(\u001b[32mFSDataInputStream.java\u001b[39m:\u001b[32m62\u001b[39m)\n  org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m330\u001b[39m)\n  io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(\u001b[32mPlinkFileFormat.scala\u001b[39m:\u001b[32m118\u001b[39m)\n  io.projectglow.plink.PlinkFileFormat$$anonfun$buildReader$1.apply(\u001b[32mPlinkFileFormat.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(\u001b[32mFileFormat.scala\u001b[39m:\u001b[32m148\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(\u001b[32mFileFormat.scala\u001b[39m:\u001b[32m132\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m124\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m177\u001b[39m)\n  org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(\u001b[32mFileScanRDD.scala\u001b[39m:\u001b[32m101\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m636\u001b[39m)\n  scala.collection.Iterator$$anon$12.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m440\u001b[39m)\n  scala.collection.Iterator$JoinIterator.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m212\u001b[39m)\n  scala.collection.Iterator$$anon$11.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m409\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m636\u001b[39m)\n  scala.collection.Iterator$$anon$11.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m409\u001b[39m)\n  org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(\u001b[32mBypassMergeSortShuffleWriter.java\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m99\u001b[39m)\n  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m123\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m408\u001b[39m)\n  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1360\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m414\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1149\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m624\u001b[39m)\n  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m748\u001b[39m)"
     ]
    }
   ],
   "source": [
    "// Frequency of call types showing that homozygous reference is most common by far\n",
    "dfc.groupBy(\"state\").count.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
