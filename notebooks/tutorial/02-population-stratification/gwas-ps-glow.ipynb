{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glow GWAS 1KG QC\n",
    "\n",
    "This tutorial will only run the initial part of the 1KG data QC.  Even this first part takes several hours so re-writing the entire tutorial this way is basically infeasible with Glow (on a single server)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/eczech/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/eczech/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/02/07 13:21:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://localhost:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.41.3.min',\n",
       "    jquery: 'https://code.jquery.com/jquery-3.3.1.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$           , spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$           , paths._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$          , glow._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$               , benchmark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$               , functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                             , init_plotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36msys.process._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrame\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mio.projectglow.Glow\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.Almond.{init => init_plotly_js, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbetter.files.File\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtimeop\u001b[39m\n",
       "\u001b[36mss\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@34ec0bf9\n",
       "\u001b[32mimport \u001b[39m\u001b[36mss.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mdata_dir\u001b[39m: \u001b[32mFile\u001b[39m = /home/eczech/data/gwas/tutorial/2_PS_GWAS\n",
       "\u001b[36mPS1_1KG_RAW_FILE\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"ALL.2of4intersection.20100804.genotypes\"\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.^.init.spark, spark._\n",
    "import $file.^.init.paths, paths._\n",
    "import $file.^.init.glow, glow._\n",
    "import $file.^.init.benchmark, benchmark._\n",
    "import $file.^.init.functions, functions._\n",
    "import $file.^.init.{plotly => init_plotly}, init_plotly._\n",
    "import sys.process._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import io.projectglow.Glow\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.Almond.{init => init_plotly_js, _}\n",
    "import better.files.File\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"io.projectglow.plink\").setLevel(Level.WARN)\n",
    "\n",
    "def timeop[T](op: String)(block: => T) = optimer(\"glow\", op, block)\n",
    "\n",
    "// Increase broadcast timeout from 300 to avoid \"Could not execute broadcast in 300 secs.\"\n",
    "val ss = getLocalSparkSession(\n",
    "    broadcastTimeoutSeconds=14400, \n",
    "    shufflePartitions=1,\n",
    "    enableUI=true\n",
    ") \n",
    "import ss.implicits._\n",
    "Glow.register(ss)\n",
    "\n",
    "init_plotly_js(offline=false)\n",
    "\n",
    "val data_dir = GWAS_TUTORIAL_DATA_DIR / \"2_PS_GWAS\"\n",
    "val PS1_1KG_RAW_FILE = \"ALL.2of4intersection.20100804.genotypes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert 1KG to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contigName: string (nullable = true)\n",
      " |-- names: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- position: double (nullable = true)\n",
      " |-- start: long (nullable = true)\n",
      " |-- end: long (nullable = true)\n",
      " |-- referenceAllele: string (nullable = true)\n",
      " |-- alternateAlleles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- genotypes: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- sampleId: string (nullable = true)\n",
      " |    |    |-- calls: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = ss.read.format(\"plink\")\n",
    "//     .option(\"famDelimiter\", \"\\t\")\n",
    "//     .option(\"bimDelimiter\", \"\\t\")\n",
    "    .load(data_dir / PS1_1KG_RAW_FILE + \".bed\" toString)\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1099.5 seconds\n"
     ]
    }
   ],
   "source": [
    "timeop(\"ps0\") {\n",
    "    df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(data_dir / PS1_1KG_RAW_FILE + \".parquet\" toString)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1KG QC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]\n",
       "\u001b[36mres1_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m25488488L\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = ss.read.parquet(data_dir / PS1_1KG_RAW_FILE + \".parquet\" toString)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|size(genotypes)|   count|\n",
      "+---------------+--------+\n",
      "|            629|25488488|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(size($\"genotypes\")).count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "// To take a sample efficiently, this appears to be the best way\n",
    "// (.limit will scan every paritition first)\n",
    "// val df = ss.read.parquet(data_dir / PS1_1KG_RAW_FILE + \".parquet\" toString)\n",
    "//     .transform(d => {ss.createDataFrame(d.takeAsList(100), d.schema)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcount\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfilterBySampleCallRate\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfilterByVariantCallRate\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// QC functions\n",
    "\n",
    "def count(df: DataFrame) = (df.count, df.select(size(col(\"genotypes\"))).first.getAs[Int](0)) // (n_variants, n_samples)\n",
    "\n",
    "def filterBySampleCallRate(threshold: Double)(df: DataFrame): DataFrame = { \n",
    "    df\n",
    "        // Cross join original dataset with single-row data frame containing a map like (sampleId -> QC stats)\n",
    "        .crossJoin(\n",
    "            df\n",
    "            .selectExpr(\"sample_call_summary_stats(genotypes, referenceAllele, alternateAlleles) as qc\")\n",
    "            .selectExpr(\"map_from_arrays(qc.sampleId, qc) as qc\")\n",
    "        )\n",
    "        // For each row, filter the genotypes array (which has one element per sampleId) based on QC map lookup\n",
    "        .selectExpr(\"*\", s\"filter(genotypes, g -> qc[g.sampleId].callRate >= ${threshold}) as filtered_genotypes\")\n",
    "        // Remove intermediate fields \n",
    "        .drop(\"qc\", \"genotypes\").withColumnRenamed(\"filtered_genotypes\", \"genotypes\")\n",
    "        // Ensure that the original dataset schema was preserved\n",
    "        .transform(d => {assert(d.schema.equals(df.schema)); d})\n",
    "}\n",
    "\n",
    "// Rewrite based on https://github.com/projectglow/glow/issues/148#issuecomment-582485763\n",
    "\n",
    "def filterByVariantCallRate(threshold: Double)(df: DataFrame): DataFrame = { \n",
    "    df\n",
    "        .selectExpr(\"*\", \"call_summary_stats(genotypes) as qc\")\n",
    "        .filter(col(\"qc.callRate\") >= threshold)\n",
    "        .drop(\"qc\")\n",
    "        .transform(d => {assert(d.schema.equals(df.schema)); d})\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Implementation**\n",
    "\n",
    "This version of the filtering methods (```filterByVariantCallRate``` in particular) was found to be suboptimal as discussed in https://github.com/projectglow/glow/issues/148#issuecomment-582485763.\n",
    "\n",
    "Nevertheless, it is left here for posterity since it my help others avoid similar anti-patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ran for ~3 hours (with both 1 and 200 shuffle partitions) and \n",
    "// used 30g of 64g heap before being killed.\n",
    "val df_qc1 = df\n",
    "    .transform(filterByVariantCallRate(threshold=.8))\n",
    "    .transform(filterBySampleCallRate(threshold=.8))\n",
    "    .transform(filterByVariantCallRate(threshold=.98))\n",
    "    .transform(filterBySampleCallRate(threshold=.98))\n",
    "\n",
    "val ct = timeop(\"ps1\") {\n",
    "    count(df_qc1)\n",
    "}\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improved Implementation**\n",
    "\n",
    "This rewrite, that avoids generating maps of sampleId -> qc objects, turns out to be much more efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Rewrite based on https://github.com/projectglow/glow/issues/148#issuecomment-582485763\n",
    "def filterBySampleCallRate(threshold: Double)(df: DataFrame): DataFrame = {\n",
    "  val qc = df.selectExpr(\"sample_call_summary_stats(genotypes, referenceAllele, alternateAlleles) as qc\")\n",
    "  df.crossJoin(qc)\n",
    "    // For each row, filter the genotypes array (which has one element per sampleId) based on whether it passed the QC filter\n",
    "    .selectExpr(\"*\", s\"\"\"\n",
    "        transform(\n",
    "            filter(\n",
    "                zip_with(sequence(0, size(genotypes)), genotypes, (i, g) -> (i, g)), e -> \n",
    "                qc[e.i].callRate >= $threshold\n",
    "            ), \n",
    "            e -> e.g\n",
    "        ) as filtered_genotypes\n",
    "    \"\"\")\n",
    "    // Remove intermediate fields \n",
    "    .drop(\"qc\", \"genotypes\")\n",
    "    .withColumnRenamed(\"filtered_genotypes\", \"genotypes\")\n",
    "    // Ensure that the original dataset schema was preserved\n",
    "    .transform(d => {assert(d.schema.equals(df.schema)); d})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 5113.1 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_qc1\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [contigName: string, names: array<string> ... 6 more fields]\n",
       "\u001b[36mct\u001b[39m: (\u001b[32mLong\u001b[39m, \u001b[32mInt\u001b[39m) = (\u001b[32m8240745L\u001b[39m, \u001b[32m629\u001b[39m)\n",
       "\u001b[36mres3_2\u001b[39m: (\u001b[32mLong\u001b[39m, \u001b[32mInt\u001b[39m) = (\u001b[32m8240745L\u001b[39m, \u001b[32m629\u001b[39m)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Try the QC again:\n",
    "\n",
    "val df_qc1 = df\n",
    "    .transform(filterByVariantCallRate(threshold=.8))\n",
    "    .transform(filterBySampleCallRate(threshold=.8))\n",
    "    .transform(filterByVariantCallRate(threshold=.98))\n",
    "    .transform(filterBySampleCallRate(threshold=.98))\n",
    "\n",
    "val ct = timeop(\"ps1\") {\n",
    "    count(df_qc1)\n",
    "}\n",
    "ct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
