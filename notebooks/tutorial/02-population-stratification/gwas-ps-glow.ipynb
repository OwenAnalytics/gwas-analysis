{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/eczech/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/eczech/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/02/06 16:26:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://localhost:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.41.3.min',\n",
       "    jquery: 'https://code.jquery.com/jquery-3.3.1.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$           , spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$           , paths._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$          , glow._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$               , benchmark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$               , functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                             , init_plotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36msys.process._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrame\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mio.projectglow.Glow\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.Almond.{init => init_plotly_js, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbetter.files.File\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtimeop\u001b[39m\n",
       "\u001b[36mss\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@2c36b5cc\n",
       "\u001b[32mimport \u001b[39m\u001b[36mss.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mdata_dir\u001b[39m: \u001b[32mFile\u001b[39m = /home/eczech/data/gwas/tutorial/2_PS_GWAS\n",
       "\u001b[36mPS1_1KG_RAW_FILE\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"ALL.2of4intersection.20100804.genotypes\"\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.^.init.spark, spark._\n",
    "import $file.^.init.paths, paths._\n",
    "import $file.^.init.glow, glow._\n",
    "import $file.^.init.benchmark, benchmark._\n",
    "import $file.^.init.functions, functions._\n",
    "import $file.^.init.{plotly => init_plotly}, init_plotly._\n",
    "import sys.process._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import io.projectglow.Glow\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.Almond.{init => init_plotly_js, _}\n",
    "import better.files.File\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"io.projectglow.plink\").setLevel(Level.WARN)\n",
    "\n",
    "def timeop[T](op: String)(block: => T) = optimer(\"glow\", op, block)\n",
    "\n",
    "// Increase broadcast timeout from 300 to avoid \"Could not execute broadcast in 300 secs.\"\n",
    "val ss = getLocalSparkSession(\n",
    "    broadcastTimeoutSeconds=14400, \n",
    "    shufflePartitions=1,\n",
    "    enableUI=true\n",
    ") \n",
    "import ss.implicits._\n",
    "Glow.register(ss)\n",
    "\n",
    "init_plotly_js(offline=false)\n",
    "\n",
    "val data_dir = GWAS_TUTORIAL_DATA_DIR / \"2_PS_GWAS\"\n",
    "//val PS1_1KG_RAW_FILE = \"ALL.2of4intersection.20100804.genotypes_no_missing_IDs\"\n",
    "val PS1_1KG_RAW_FILE = \"ALL.2of4intersection.20100804.genotypes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert 1KG to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contigName: string (nullable = true)\n",
      " |-- names: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- position: double (nullable = true)\n",
      " |-- start: long (nullable = true)\n",
      " |-- end: long (nullable = true)\n",
      " |-- referenceAllele: string (nullable = true)\n",
      " |-- alternateAlleles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- genotypes: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- sampleId: string (nullable = true)\n",
      " |    |    |-- calls: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = ss.read.format(\"plink\")\n",
    "//     .option(\"famDelimiter\", \"\\t\")\n",
    "//     .option(\"bimDelimiter\", \"\\t\")\n",
    "    .load(data_dir / PS1_1KG_RAW_FILE + \".bed\" toString)\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1099.5 seconds\n"
     ]
    }
   ],
   "source": [
    "timeop(\"ps0\") {\n",
    "    df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(data_dir / PS1_1KG_RAW_FILE + \".parquet\" toString)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1KG QC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [contigName: string, names: array<string> ... 6 more fields]\n",
       "\u001b[36mres7_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m25488488L\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = ss.read.parquet(data_dir / PS1_1KG_RAW_FILE + \".parquet\" toString)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|size(genotypes)|   count|\n",
      "+---------------+--------+\n",
      "|            629|25488488|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(size($\"genotypes\")).count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|size(genotypes)|   count|\n",
      "+---------------+--------+\n",
      "|            629|25488488|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(size($\"genotypes\")).count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "// To take a sample efficiently, this appears to be the best way\n",
    "// (.limit will scan every paritition first)\n",
    "// val df = ss.read.parquet(data_dir / PS1_1KG_RAW_FILE + \".parquet\" toString)\n",
    "//     .transform(d => {ss.createDataFrame(d.takeAsList(100), d.schema)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcount\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfilterBySampleCallRate\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfilterByVariantCallRate\u001b[39m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// QC functions\n",
    "\n",
    "def count(df: DataFrame) = (df.count, df.select(size(col(\"genotypes\"))).first.getAs[Int](0)) // (n_variants, n_samples)\n",
    "\n",
    "// def filterBySampleCallRate(threshold: Double)(df: DataFrame): DataFrame = { \n",
    "//     df\n",
    "//         // Cross join original dataset with single-row data frame containing a map like (sampleId -> QC stats)\n",
    "//         .crossJoin(\n",
    "//             df\n",
    "//             .selectExpr(\"sample_call_summary_stats(genotypes, referenceAllele, alternateAlleles) as qc\")\n",
    "//             .selectExpr(\"map_from_arrays(qc.sampleId, qc) as qc\")\n",
    "//         )\n",
    "//         // For each row, filter the genotypes array (which has one element per sampleId) based on QC map lookup\n",
    "//         .selectExpr(\"*\", s\"filter(genotypes, g -> qc[g.sampleId].callRate >= ${threshold}) as filtered_genotypes\")\n",
    "//         // Remove intermediate fields \n",
    "//         .drop(\"qc\", \"genotypes\").withColumnRenamed(\"filtered_genotypes\", \"genotypes\")\n",
    "//         // Ensure that the original dataset schema was preserved\n",
    "//         .transform(d => {assert(d.schema.equals(df.schema)); d})\n",
    "// }\n",
    "\n",
    "// Rewrite based on https://github.com/projectglow/glow/issues/148#issuecomment-582485763\n",
    "def filterBySampleCallRate(threshold: Double)(df: DataFrame): DataFrame = {\n",
    "  val qc = df.selectExpr(\"sample_call_summary_stats(genotypes, referenceAllele, alternateAlleles) as qc\")\n",
    "  df.crossJoin(qc)\n",
    "    // For each row, filter the genotypes array (which has one element per sampleId) based on whether it passed the QC filter\n",
    "    .selectExpr(\"*\", s\"\"\"\n",
    "        transform(\n",
    "            filter(\n",
    "                zip_with(sequence(0, size(genotypes)), genotypes, (i, g) -> (i, g)), e -> \n",
    "                qc[e.i].callRate >= $threshold\n",
    "            ), \n",
    "            e -> e.g\n",
    "        ) as filtered_genotypes\n",
    "    \"\"\")\n",
    "    // Remove intermediate fields \n",
    "    .drop(\"qc\", \"genotypes\")\n",
    "    .withColumnRenamed(\"filtered_genotypes\", \"genotypes\")\n",
    "    // Ensure that the original dataset schema was preserved\n",
    "    .transform(d => {assert(d.schema.equals(df.schema)); d})\n",
    "}\n",
    "\n",
    "def filterByVariantCallRate(threshold: Double)(df: DataFrame): DataFrame = { \n",
    "    df\n",
    "        .selectExpr(\"*\", \"call_summary_stats(genotypes) as qc\")\n",
    "        .filter(col(\"qc.callRate\") >= threshold)\n",
    "        .drop(\"qc\")\n",
    "        .transform(d => {assert(d.schema.equals(df.schema)); d})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Interrupted!\n  sun.misc.Unsafe.park(\u001b[32mNative Method\u001b[39m)\n  java.util.concurrent.locks.LockSupport.parkNanos(\u001b[32mLockSupport.java\u001b[39m:\u001b[32m215\u001b[39m)\n  java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(\u001b[32mAbstractQueuedSynchronizer.java\u001b[39m:\u001b[32m1037\u001b[39m)\n  java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(\u001b[32mAbstractQueuedSynchronizer.java\u001b[39m:\u001b[32m1328\u001b[39m)\n  scala.concurrent.impl.Promise$DefaultPromise.tryAwait(\u001b[32mPromise.scala\u001b[39m:\u001b[32m212\u001b[39m)\n  scala.concurrent.impl.Promise$DefaultPromise.ready(\u001b[32mPromise.scala\u001b[39m:\u001b[32m222\u001b[39m)\n  scala.concurrent.impl.Promise$DefaultPromise.result(\u001b[32mPromise.scala\u001b[39m:\u001b[32m227\u001b[39m)\n  org.apache.spark.util.ThreadUtils$.awaitResult(\u001b[32mThreadUtils.scala\u001b[39m:\u001b[32m220\u001b[39m)\n  org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(\u001b[32mBroadcastExchangeExec.scala\u001b[39m:\u001b[32m146\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m144\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m140\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeBroadcast(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m140\u001b[39m)\n  org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.doExecute(\u001b[32mBroadcastNestedLoopJoinExec.scala\u001b[39m:\u001b[32m343\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.InputAdapter.inputRDDs(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m391\u001b[39m)\n  org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(\u001b[32mHashAggregateExec.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m627\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m92\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m128\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m119\u001b[39m)\n  org.apache.spark.sql.catalyst.errors.package$.attachTree(\u001b[32mpackage.scala\u001b[39m:\u001b[32m52\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m119\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.InputAdapter.inputRDDs(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m391\u001b[39m)\n  org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(\u001b[32mHashAggregateExec.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m627\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m247\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeCollect(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m296\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$count$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2836\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$count$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2835\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$52.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3370\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3369\u001b[39m)\n  org.apache.spark.sql.Dataset.count(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2835\u001b[39m)\n  ammonite.$sess.cmd20$Helper.count(\u001b[32mcmd20.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd21$Helper$$anonfun$5.apply(\u001b[32mcmd21.sc\u001b[39m:\u001b[32m8\u001b[39m)\n  ammonite.$sess.cmd21$Helper$$anonfun$5.apply(\u001b[32mcmd21.sc\u001b[39m:\u001b[32m8\u001b[39m)\n  ammonite.$sess.$up.init.benchmark$Helper.optimer(\u001b[32mbenchmark.sc\u001b[39m:\u001b[32m12\u001b[39m)\n  ammonite.$sess.cmd0$Helper.timeop(\u001b[32mcmd0.sc\u001b[39m:\u001b[32m19\u001b[39m)\n  ammonite.$sess.cmd21$Helper.<init>(\u001b[32mcmd21.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd21$.<init>(\u001b[32mcmd21.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd21$.<clinit>(\u001b[32mcmd21.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "// Ran for ~3 hours (with both 1 and 200 shuffle partitions) and \n",
    "// used 30g of 64g heap before being killed.\n",
    "val df_qc1 = df\n",
    "    .transform(filterByVariantCallRate(threshold=.8))\n",
    "    .transform(filterBySampleCallRate(threshold=.8))\n",
    "    .transform(filterByVariantCallRate(threshold=.98))\n",
    "    .transform(filterBySampleCallRate(threshold=.98))\n",
    "\n",
    "val ct = timeop(\"ps1\") {\n",
    "    count(df_qc1)\n",
    "}\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liftover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Get hg18 reference:\n",
    "- Download ```chromFa.zip``` from http://hgdownload.cse.ucsc.edu/goldenPath/hg18/bigZips/chromFa.zip\n",
    "- See notes and other downloads at same location: http://hgdownload.cse.ucsc.edu/goldenpath/hg18/chromosomes/\n",
    "- Unzip and run ```rm *_random*``` to delete ```chrN_random.fa``` files\n",
    "    - \"The chrN_random.fa files contain clones that are not yet finished or cannot be placed with certainty at a specific place on the chromosome\"\n",
    "- Concatenate all the per-chromosome files together: ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
