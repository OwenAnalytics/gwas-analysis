{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH Data Generator\n",
    "\n",
    "Exports PLINK, parquet, and zarr datasets for a single dataset (currently either simulated or sampled from HapMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 2.4.4\n",
      "SparkUI available at http://2e4e0c6972f9:4043\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.32-a5876a0a2853\n",
      "LOGGING: writing to /home/eczech/repos/gwas-analysis/notebooks/benchmark/method/ld_prune/lsh/hail-20200223-2350-0.2.32-a5876a0a2853.log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hail as hl\n",
    "import numpy as np\n",
    "from gwas_analysis.dask import io\n",
    "from pysnptools import snpreader\n",
    "import gwas_analysis.simulation.datasets as gsd\n",
    "import dask.array as da\n",
    "%run {os.environ['NB_DIR']}/nb.py\n",
    "%run $BENCHMARK_METHOD_DIR/common.py\n",
    "\n",
    "sample_rate = .1\n",
    "ds_name = DATASET_HM\n",
    "\n",
    "# sample_rate = 1\n",
    "# ds_name = DATASET_SIM\n",
    "\n",
    "ds_config = DATASET_CONFIG[ds_name]\n",
    "ds_export_path = dataset_path(ds_name, sr=sample_rate)\n",
    "hail_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 23:51:03 Hail: INFO: Found 165 samples in fam file.\n",
      "2020-02-23 23:51:03 Hail: INFO: Found 1457897 variants in bim file.\n",
      "2020-02-23 23:51:11 Hail: INFO: Coerced sorted dataset\n",
      "2020-02-23 23:51:11 Hail: INFO: reading 1 of 2 data partitions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before removing rows with no variance: (12109, 165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 23:51:18 Hail: INFO: Coerced sorted dataset\n",
      "2020-02-23 23:51:18 Hail: INFO: reading 1 of 2 data partitions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10451, 165)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ds_name == DATASET_SIM:\n",
    "    # Make sure a single contig is used for comparison to Hail results\n",
    "    # (Hail returns 0s for variants on unequal contigs)\n",
    "    mt = gsd.get_ldsim_dataset(n_variants=256, n_samples=6, n_contigs=1, seed=1)\n",
    "else:\n",
    "    mt = hl.import_plink(\n",
    "        *plink_files(osp.dirname(ds_config['path']), osp.basename(ds_config['path'])),\n",
    "        skip_invalid_loci=False,\n",
    "        reference_genome=ds_config['reference_genome']\n",
    "    )\n",
    "    mt = mt.filter_rows(mt.locus.contig == '1')\n",
    "\n",
    "mt = mt.sample_rows(p=sample_rate, seed=1)\n",
    "\n",
    "print('Shape before removing rows with no variance:', mt.count())\n",
    "mt = mt.annotate_rows(stdev=hl.agg.stats(mt.GT.n_alt_alleles()).stdev)\n",
    "mt = mt.filter_rows(mt.stdev > 0)\n",
    "\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Plink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 23:51:36 Hail: INFO: Coerced sorted dataset\n",
      "2020-02-23 23:51:36 Hail: INFO: reading 1 of 2 data partitions\n",
      "2020-02-23 23:51:41 Hail: INFO: Coerced sorted dataset\n",
      "2020-02-23 23:51:41 Hail: INFO: reading 1 of 2 data partitions\n",
      "2020-02-23 23:51:46 Hail: INFO: Coerced sorted dataset\n",
      "2020-02-23 23:51:46 Hail: INFO: reading 1 of 2 data partitions\n",
      "2020-02-23 23:51:51 Hail: INFO: Coerced sorted dataset\n",
      "2020-02-23 23:51:51 Hail: INFO: reading 1 of 2 data partitions\n",
      "2020-02-23 23:51:56 Hail: INFO: Coerced sorted dataset\n",
      "2020-02-23 23:51:56 Hail: INFO: reading 1 of 2 data partitions\n",
      "2020-02-23 23:52:04 Hail: INFO: merging 2 files totalling 428.7K...\n",
      "2020-02-23 23:52:04 Hail: INFO: while writing:\n",
      "    /home/eczech/data/gwas/benchmark/datasets/hapmap-sr=0.1.bed\n",
      "  merge time: 23.359ms\n",
      "2020-02-23 23:52:04 Hail: INFO: merging 1 files totalling 303.6K...\n",
      "2020-02-23 23:52:04 Hail: INFO: while writing:\n",
      "    /home/eczech/data/gwas/benchmark/datasets/hapmap-sr=0.1.bim\n",
      "  merge time: 16.373ms\n",
      "2020-02-23 23:52:05 Hail: INFO: merging 16 files totalling 4.0K...\n",
      "2020-02-23 23:52:05 Hail: INFO: while writing:\n",
      "    /home/eczech/data/gwas/benchmark/datasets/hapmap-sr=0.1.fam\n",
      "  merge time: 9.343ms\n",
      "2020-02-23 23:52:05 Hail: INFO: wrote 10451 variants and 165 samples to '/home/eczech/data/gwas/benchmark/datasets/hapmap-sr=0.1'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/eczech/data/gwas/benchmark/datasets/hapmap-sr=0.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def export(mt, path):\n",
    "    hl.export_plink(\n",
    "        mt, path, \n",
    "        fam_id=mt.fam_id,\n",
    "        pat_id=mt.pat_id,\n",
    "        mat_id=mt.mat_id,\n",
    "        is_female=mt.is_female,\n",
    "        pheno=mt.is_case,\n",
    "        varid=mt.rsid\n",
    "    )\n",
    "export(mt, ds_export_path)\n",
    "ds_export_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 23:52:10 Hail: INFO: Coerced sorted dataset\n",
      "2020-02-23 23:52:10 Hail: INFO: reading 1 of 2 data partitions\n",
      "2020-02-23 23:52:26 Hail: INFO: Wrote all 3 blocks of 10451 x 165 matrix with block size 4096.\n"
     ]
    }
   ],
   "source": [
    "# Note: mean imputation might be useful here \n",
    "bm = hl.linalg.BlockMatrix.from_entry_expr(hl.coalesce(mt.GT.n_alt_alleles(), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-23 23:52:26 Hail: INFO: wrote matrix with 10451 rows and 165 columns as 3 blocks of size 4096 to file:/tmp/hail.zOABc1UrD7Yv/xs06FhFey6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'row_idx': int64 \n",
      "    'entries': array<float64> \n",
      "----------------------------------------\n",
      "Key: ['row_idx']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "bt = bm.to_table_row_major()\n",
    "bt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452K\t/home/eczech/data/gwas/benchmark/datasets/hapmap-sr=0.1.parquet\n",
      "452K\ttotal\n"
     ]
    }
   ],
   "source": [
    "path = ds_export_path + '.parquet'\n",
    "bt.to_spark().write.parquet(path, mode='overwrite')\n",
    "!du -ch $path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:36565</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>120.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:36565' processes=4 threads=4, memory=120.00 GB>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = get_dask_client(n_workers=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 1.72 MB </td> <td> 1.72 MB </td></tr>\n",
       "    <tr><th> Shape </th><td> (10451, 165) </td> <td> (10451, 165) </td></tr>\n",
       "    <tr><th> Count </th><td> 4 Tasks </td><td> 1 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> int8 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"77\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"27\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"27\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"27\" y1=\"0\" x2=\"27\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 27.730928,0.000000 27.730928,120.000000 0.000000,120.000000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"13.865464\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >165</text>\n",
       "  <text x=\"47.730928\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,47.730928,60.000000)\">10451</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<sub, shape=(10451, 165), dtype=int8, chunksize=(10451, 165), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = da.from_array(io.BedArray(snpreader.Bed(ds_export_path, count_A1=True)), lock=False)\n",
    "# Convert 0=missing, 1=homo ref, etc to -1=missing, 0=homo ref\n",
    "gt = gt.astype(np.int8) - 1\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  0,  1,  2], dtype=int8),\n",
       " array([   4195, 1066270,  528602,  125348]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(gt.compute(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "828K\t/home/eczech/data/gwas/benchmark/datasets/hapmap-sr=0.1.zarr\n",
      "828K\ttotal\n"
     ]
    }
   ],
   "source": [
    "path = ds_export_path + '.zarr'\n",
    "gt.to_zarr(path, overwrite=True)\n",
    "!du -ch $path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
